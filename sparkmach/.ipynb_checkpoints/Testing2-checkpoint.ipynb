{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Author: Gusseppe Bravo <gbravor@uni.pe>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\"\"\"\n",
    "This module provides the logic of the whole project.\n",
    "\n",
    "\"\"\"\n",
    "import define\n",
    "#import analyze\n",
    "import prepare\n",
    "import feature_selection\n",
    "import evaluate\n",
    "\n",
    "import time\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark import SparkContext, SparkConf\n",
    "\n",
    "try:\n",
    "#     spark.stop()\n",
    "    pass\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "name = \"datasets/buses_10000_filtered.csv\"\n",
    "#     name = \"hdfs://King:9000/user/bdata/mta_data/MTA-Bus-Time_.2014-08-01.txt\"\n",
    "className = \"tiempoRecorrido\"\n",
    "\n",
    "sparkSession = SparkSession.builder \\\n",
    ".master('local')\\\n",
    ".appName(\"Sparkmach\") \\\n",
    ".config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    ".getOrCreate()\n",
    "    \n",
    "    \n",
    "# conf = SparkConf()\\\n",
    "# .setMaster(\"local\")\\\n",
    "# .setAppName(\"sparkmach\")\\\n",
    "# .set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "\n",
    "#sparkContext = SparkContext(conf=conf)\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/define.py\")\n",
    "#sparkSession.sparkContext.addPyFile(\"/home/vagrant/tesis/sparkmach/sparkmach/sparkmach/analyze.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/prepare.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/feature_selection.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/evaluate.py\")\n",
    "\n",
    "# STEP 0: Define workflow parameters\n",
    "definer = define.Define(sparkSession, nameData=name, className=className).pipeline()\n",
    "\n",
    "# STEP 1: Analyze data by ploting it\n",
    "#analyze.Analyze(definer).pipeline()\n",
    "\n",
    "# STEP 2: Prepare data by scaling, normalizing, etc. \n",
    "preparer = prepare.Prepare(definer).pipeline()\n",
    "\n",
    "#STEP 3: Feature selection\n",
    "featurer = feature_selection.FeatureSelection(definer).pipeline()\n",
    "\n",
    "#STEP4: Evalute the algorithms by using the pipelines\n",
    "# evaluator = evaluate.Evaluate(definer, preparer, featurer).pipeline()\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# result = main()\n",
    "# end = time.time()\n",
    "\n",
    "# print()\n",
    "# print(\"Execution time for all the steps: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+------------------+\n",
      "|Model                      |Score               |Time              |\n",
      "+---------------------------+--------------------+------------------+\n",
      "|DecisionTreeRegressor      |0.08964561627029533 |6.327453374862671 |\n",
      "|GeneralizedLinearRegression|0.013625352415245873|5.869198322296143 |\n",
      "|Total time                 |0.0                 |12.196651697158813|\n",
      "+---------------------------+--------------------+------------------+\n",
      "\n",
      "CPU times: user 444 ms, sys: 66.1 ms, total: 510 ms\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = evaluate.Evaluate(definer, preparer, featurer).pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>busID</th>\n",
       "      <th>ProximaParada</th>\n",
       "      <th>Ruta</th>\n",
       "      <th>Orientacion</th>\n",
       "      <th>rangoHora</th>\n",
       "      <th>tiempoRecorrido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3433</td>\n",
       "      <td>249</td>\n",
       "      <td>9735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3433</td>\n",
       "      <td>249</td>\n",
       "      <td>9735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3984</td>\n",
       "      <td>160</td>\n",
       "      <td>12237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3984</td>\n",
       "      <td>160</td>\n",
       "      <td>12237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5517</td>\n",
       "      <td>255</td>\n",
       "      <td>12521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   busID  ProximaParada   Ruta  Orientacion  rangoHora  tiempoRecorrido\n",
       "0   3433            249   9735          1.0        2.0            856.0\n",
       "1   3433            249   9735          1.0        5.0            636.0\n",
       "2   3984            160  12237          1.0        5.0             96.0\n",
       "3   3984            160  12237          1.0        7.5             32.0\n",
       "4   5517            255  12521          0.0       11.5             31.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_data = pd.read_csv('datasets/buses_1458098_filtered.csv')\n",
    "bus_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1449200, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.14:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sparkmach</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Sparkmach>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| ID|  c|  b|  a|\n",
      "+---+---+---+---+\n",
      "|  1|  0|  0|  1|\n",
      "|  2|  0|  1|  0|\n",
      "|  3|  1|  0|  0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = sparkSession.createDataFrame([\n",
    "    (1, \"a\"),\n",
    "    (2, \"b\"),\n",
    "    (3, \"c\"),\n",
    "], [\"ID\", \"Text\"])\n",
    "\n",
    "categories = df.select(\"Text\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "exprs = [F.when(F.col(\"Text\") == category, 1).otherwise(0).alias(category)\n",
    "         for category in categories]\n",
    "\n",
    "df.select(\"ID\", *exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'b', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(df.select(\"Text\").distinct().rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+\n",
      "|Model                      |Score               |\n",
      "+---------------------------+--------------------+\n",
      "|GBTRegressor               |0.18272008210363277 |\n",
      "|RandomForestRegressor      |0.12066847476050002 |\n",
      "|DecisionTreeRegressor      |0.08964561627029533 |\n",
      "|LinearRegression           |0.013625352415245873|\n",
      "|GeneralizedLinearRegression|0.013625352415245873|\n",
      "+---------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.sort(col(\"Score\").desc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+\n",
      "|Model                      |Score               |\n",
      "+---------------------------+--------------------+\n",
      "|LinearRegression           |0.013625352415245873|\n",
      "|GeneralizedLinearRegression|0.013625352415245873|\n",
      "|DecisionTreeRegressor      |0.08964561627029533 |\n",
      "|RandomForestRegressor      |0.12066847476050002 |\n",
      "|GBTRegressor               |0.18272008210363277 |\n",
      "+---------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Piero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import bus_times\n",
    "import os\n",
    "import define\n",
    "#import analyze\n",
    "import prepare\n",
    "import feature_selection\n",
    "import evaluate\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "name = \"datasets/buses_10000_filtered.csv\"\n",
    "#     name = \"hdfs://King:9000/user/bdata/mta_data/MTA-Bus-Time_.2014-08-01.txt\"\n",
    "className = \"class\"\n",
    "\n",
    "\n",
    "sparkSession = SparkSession.builder \\\n",
    ".master('local[*]')\\\n",
    ".appName(\"Sparkmach\") \\\n",
    ".config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "#Piero\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/bus_times.py\") \n",
    "\n",
    "#Gusseppe\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/define.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/prepare.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/feature_selection.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/evaluate.py\")\n",
    "\n",
    "rdd = sparkSession.sparkContext.textFile(currentDir + '/datasets/MTA-Bus-Time_.2014-08-01.txt')\n",
    "# rdd = sparkSession.sparkContext.textFile(currentDir + '/datasets/test.txt')\n",
    "# rdd = sc.textFile('hdfs://King:9000/user/bdata/mta_data/MTA-Bus-Time_.2014-10-31.txt')\n",
    "\n",
    "\n",
    "classTuple= bus_times.mainFilter(rdd)\n",
    "halfHourBucket=classTuple.map(lambda x: bus_times.toHalfHourBucket(list(x)))\n",
    "\n",
    "\n",
    "bucket_schema= StructType([StructField(\"bus_id\",StringType(), True),StructField(\"route_id\",StringType(), True),StructField(\"next_stop_id\",StringType(), True),StructField(\"direction\",StringType(), True),StructField(\"half_hour_bucket\",StringType(), True),StructField(\"class\",StringType(), True) ])\n",
    "# bucket_schema= StructType([StructField(\"bus_id\",IntegerType(), True),StructField(\"route_id\",StringType(), True),StructField(\"next_stop_id\",StringType(), True),StructField(\"direction\",IntegerType(), True),StructField(\"half_hour_bucket\",FloatType(), True),StructField(\"class\",FloatType(), True) ])\n",
    "\n",
    "df = sparkSession.createDataFrame(halfHourBucket, bucket_schema)\n",
    "stringIndexer = StringIndexer(inputCol='route_id', outputCol='route_id'+\"_Index\")\n",
    "df = stringIndexer.fit(df).transform(df)    \n",
    "\n",
    "stringIndexer = StringIndexer(inputCol='next_stop_id', outputCol='next_stop_id'+\"_Index\")\n",
    "df = stringIndexer.fit(df).transform(df)    \n",
    "drop_list = ['route_id', 'next_stop_id']\n",
    "\n",
    "df = df.select([column for column in df.columns if column not in drop_list])\n",
    "\n",
    "\n",
    "# print('hellllooo')\n",
    "\n",
    "# STEP 0: Define workflow parameters\n",
    "definer = define.Define(sparkSession, nameData=name, className=className, df=df).pipeline()\n",
    "\n",
    "# STEP 1: Analyze data by ploting it\n",
    "#analyze.Analyze(definer).pipeline()\n",
    "\n",
    "# STEP 2: Prepare data by scaling, normalizing, etc. \n",
    "preparer = prepare.Prepare(definer).pipeline()\n",
    "\n",
    "#STEP 3: Feature selection\n",
    "featurer = feature_selection.FeatureSelection(definer).pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Nothing has been added to this summarizer.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o282.fit.\n: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.variance(MultivariateOnlineSummarizer.scala:204)\n\tat org.apache.spark.mllib.feature.StandardScaler.fit(StandardScaler.scala:61)\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:117)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/core/sparkmach/sparkmach/evaluate.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#evaluators = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildPipelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefineAlgorithms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluatePipelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;31m#self.setBestPipelines()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/core/sparkmach/sparkmach/evaluate.py\u001b[0m in \u001b[0;36mevaluatePipelines\u001b[0;34m(self, ax)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparamMap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparamMap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Nothing has been added to this summarizer.'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = evaluate.Evaluate(definer, preparer, featurer).pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-120f76ddb124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "sparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringIndexer = StringIndexer(inputCol='route_id', outputCol='route_id'+\"_Index\")\n",
    "# df = stringIndexer.fit(times_df).transform(times_df)    \n",
    "\n",
    "# stringIndexer = StringIndexer(inputCol='next_stop_id', outputCol='next_stop_id'+\"_Index\")\n",
    "# result = stringIndexer.fit(df).transform(df)    \n",
    "# drop_list = ['route_id', 'next_stop_id']\n",
    "\n",
    "# df = df.select([column for column in df.columns if column not in drop_list])\n",
    "\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RandomForestClassificationModel.load(\"./models/name.ml\")\n",
    "\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-81601c7341cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator.Evaluate.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(bus_id,FloatType,true),\n",
       " StructField(direction,FloatType,true),\n",
       " StructField(half_hour_bucket,FloatType,true),\n",
       " StructField(class,FloatType,true),\n",
       " StructField(route_id_Index,DoubleType,true),\n",
       " StructField(next_stop_id_Index,DoubleType,true)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definer.data.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449761"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definer.data.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449761"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = definer.data.dropna()\n",
    "tt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'definer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eb73c5e8922b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdefiner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'definer' is not defined"
     ]
    }
   ],
   "source": [
    "definer.data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(bus_id,StringType,true),StructField(direction,StringType,true),StructField(half_hour_bucket,StringType,true),StructField(class,FloatType,true),StructField(route_id_Index,DoubleType,true)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df  = df.withColumn(\"class\", col(\"class\").cast('float'))\n",
    "df  = df.withColumn(\"bus_id\", col(\"class\").cast('float'))\n",
    "df  = df.withColumn(\"direction\", col(\"class\").cast('float'))\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+-----+--------------+\n",
      "|bus_id|direction|half_hour_bucket|class|route_id_Index|\n",
      "+------+---------+----------------+-----+--------------+\n",
      "|  4367|        1|             9.0|  0.0|         157.0|\n",
      "|  4367|        1|            10.0|157.0|         157.0|\n",
      "|  4991|        1|             9.0| 63.0|         119.0|\n",
      "|   230|        0|             9.0| 32.0|           9.0|\n",
      "+------+---------+----------------+-----+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rawdata.select(col('house name'), rawdata.price.cast('float').alias('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458098"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_df.select('next_stop_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_df.select('route_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# times_df.write.csv('datasets/bus_2014-08-01.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = sparkSession.read\\\n",
    "                .format(\"txt\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "                .option(\"delimiter\", \"\\t\")\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .csv('datasets/MTA-Bus-Time_.2014-08-01.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458098"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+---------+----------------+-----+\n",
      "|bus_id|     route_id|next_stop_id|direction|half_hour_bucket|class|\n",
      "+------+-------------+------------+---------+----------------+-----+\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|             9.0|  0.0|\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|            10.0|157.0|\n",
      "|  4991| MTA NYCT_B83|  MTA_308528|        1|             9.0| 63.0|\n",
      "|   230| MTA NYCT_B35|  MTA_302697|        0|             9.0| 32.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|             9.0| 31.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|            10.5|    0|\n",
      "|  3742|    MTABC_Q10|  MTA_550316|        1|             9.0| 32.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             9.0| 31.0|\n",
      "+------+-------------+------------+---------+----------------+-----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times_df_t = times_df.limit(40)\n",
    "times_df_t.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|     route_id|next_stop_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|             9.0|  0.0|           7.0|               5.0|\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|            10.0|157.0|           7.0|               5.0|\n",
      "|  4991| MTA NYCT_B83|  MTA_308528|        1|             9.0| 63.0|          14.0|              10.0|\n",
      "|   230| MTA NYCT_B35|  MTA_302697|        0|             9.0| 32.0|          10.0|              17.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|             9.0| 31.0|           3.0|               8.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|            10.5|    0|           3.0|               8.0|\n",
      "|  3742|    MTABC_Q10|  MTA_550316|        1|             9.0| 32.0|           9.0|              12.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             9.0| 31.0|           4.0|               3.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             1.0|728.0|           4.0|               3.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             3.0|250.0|           4.0|               3.0|\n",
      "|   393| MTA NYCT_B68|  MTA_305700|        0|             9.0| 31.0|          13.0|              14.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             9.0|  0.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|            16.5| 95.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             3.0| 93.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             6.5| 31.0|           2.0|               2.0|\n",
      "|  3706|    MTABC_Q52|  MTA_550441|        0|             9.0| 62.0|          11.0|              15.0|\n",
      "|  4762|MTA NYCT_BX40|  MTA_103525|        1|             9.0| 62.0|          12.0|              18.0|\n",
      "|  3400|   MTABC_BXM9|  MTA_450082|        0|             9.0|655.0|           6.0|               7.0|\n",
      "|  3400|   MTABC_BXM9|  MTA_450082|        0|             6.0|988.0|           6.0|               7.0|\n",
      "|  6365| MTA NYCT_M10|  MTA_401335|        1|             9.0| 32.0|           3.0|              13.0|\n",
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol='route_id', outputCol='route_id'+\"_Index\")\n",
    "result = stringIndexer.fit(times_df_t).transform(times_df_t)    \n",
    "\n",
    "stringIndexer = StringIndexer(inputCol='next_stop_id', outputCol='next_stop_id'+\"_Index\")\n",
    "result = stringIndexer.fit(result).transform(result)    \n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|     route_id|next_stop_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|             9.0|  0.0|           7.0|               5.0|\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|            10.0|157.0|           7.0|               5.0|\n",
      "|  4991| MTA NYCT_B83|  MTA_308528|        1|             9.0| 63.0|          14.0|              10.0|\n",
      "|   230| MTA NYCT_B35|  MTA_302697|        0|             9.0| 32.0|          10.0|              17.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|             9.0| 31.0|           3.0|               8.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|            10.5|    0|           3.0|               8.0|\n",
      "|  3742|    MTABC_Q10|  MTA_550316|        1|             9.0| 32.0|           9.0|              12.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             9.0| 31.0|           4.0|               3.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             1.0|728.0|           4.0|               3.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             3.0|250.0|           4.0|               3.0|\n",
      "|   393| MTA NYCT_B68|  MTA_305700|        0|             9.0| 31.0|          13.0|              14.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             9.0|  0.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|            16.5| 95.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             3.0| 93.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             6.5| 31.0|           2.0|               2.0|\n",
      "|  3706|    MTABC_Q52|  MTA_550441|        0|             9.0| 62.0|          11.0|              15.0|\n",
      "|  4762|MTA NYCT_BX40|  MTA_103525|        1|             9.0| 62.0|          12.0|              18.0|\n",
      "|  3400|   MTABC_BXM9|  MTA_450082|        0|             9.0|655.0|           6.0|               7.0|\n",
      "|  3400|   MTABC_BXM9|  MTA_450082|        0|             6.0|988.0|           6.0|               7.0|\n",
      "|  6365| MTA NYCT_M10|  MTA_401335|        1|             9.0| 32.0|           3.0|              13.0|\n",
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "|  4367|        1|             9.0|  0.0|           7.0|               5.0|\n",
      "|  4367|        1|            10.0|157.0|           7.0|               5.0|\n",
      "|  4991|        1|             9.0| 63.0|          14.0|              10.0|\n",
      "|   230|        0|             9.0| 32.0|          10.0|              17.0|\n",
      "|  3834|        0|             9.0| 31.0|           3.0|               8.0|\n",
      "|  3834|        0|            10.5|    0|           3.0|               8.0|\n",
      "|  3742|        1|             9.0| 32.0|           9.0|              12.0|\n",
      "|  6296|        1|             9.0| 31.0|           4.0|               3.0|\n",
      "|  6296|        1|             1.0|728.0|           4.0|               3.0|\n",
      "|  6296|        1|             3.0|250.0|           4.0|               3.0|\n",
      "|   393|        0|             9.0| 31.0|          13.0|              14.0|\n",
      "|   496|        0|             9.0|  0.0|           2.0|               2.0|\n",
      "|   496|        0|            16.5| 95.0|           2.0|               2.0|\n",
      "|   496|        0|             3.0| 93.0|           2.0|               2.0|\n",
      "|   496|        0|             6.5| 31.0|           2.0|               2.0|\n",
      "|  3706|        0|             9.0| 62.0|          11.0|              15.0|\n",
      "|  4762|        1|             9.0| 62.0|          12.0|              18.0|\n",
      "|  3400|        0|             9.0|655.0|           6.0|               7.0|\n",
      "|  3400|        0|             6.0|988.0|           6.0|               7.0|\n",
      "|  6365|        1|             9.0| 32.0|           3.0|              13.0|\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_list = ['route_id', 'next_stop_id']\n",
    "\n",
    "result = result.select([column for column in result.columns if column not in drop_list])\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.2581, 0.7419]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9186, 0.0814]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.432, 0.568]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.6766, 0.3234]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = sparkSession.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = sparkSession.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.save('./models/bm.ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = CrossValidator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_4ef7b1a9cd490897dff4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = PipelineModel.load('./models/bm.ml')\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|           text|             words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|    spark i j k|  [spark, i, j, k]|(100,[5,29,49,56]...|[-1.0560322733153...|[0.25806842225846...|       1.0|\n",
      "|  5|          l m n|         [l, m, n]|(100,[6,38,55],[1...|[2.42293721495130...|[0.91855974126539...|       0.0|\n",
      "|  6|mapreduce spark|[mapreduce, spark]|(100,[5,53],[1.0,...|[-0.2735651887090...|[0.43203205663918...|       1.0|\n",
      "|  7|  apache hadoop|  [apache, hadoop]|(100,[81,95],[1.0...|[0.73822817597493...|[0.67660828566522...|       0.0|\n",
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp.transform(test).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|           text|             words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|    spark i j k|  [spark, i, j, k]|(100,[5,29,49,56]...|[-1.0560322733153...|[0.25806842225846...|       1.0|\n",
      "|  5|          l m n|         [l, m, n]|(100,[6,38,55],[1...|[2.42293721495130...|[0.91855974126539...|       0.0|\n",
      "|  6|mapreduce spark|[mapreduce, spark]|(100,[5,53],[1.0,...|[-0.2735651887090...|[0.43203205663918...|       1.0|\n",
      "|  7|  apache hadoop|  [apache, hadoop]|(100,[81,95],[1.0...|[0.73822817597493...|[0.67660828566522...|       0.0|\n",
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
