{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Author: Gusseppe Bravo <gbravor@uni.pe>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\"\"\"\n",
    "This module provides the logic of the whole project.\n",
    "\n",
    "\"\"\"\n",
    "import define\n",
    "#import analyze\n",
    "import prepare\n",
    "import feature_selection\n",
    "import evaluate\n",
    "\n",
    "import time\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark import SparkContext, SparkConf\n",
    "\n",
    "try:\n",
    "#     spark.stop()\n",
    "    pass\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "name = \"datasets/buses_10000_filtered.csv\"\n",
    "#     name = \"hdfs://King:9000/user/bdata/mta_data/MTA-Bus-Time_.2014-08-01.txt\"\n",
    "className = \"tiempoRecorrido\"\n",
    "\n",
    "sparkSession = SparkSession.builder \\\n",
    ".master('local')\\\n",
    ".appName(\"Sparkmach\") \\\n",
    ".config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    ".getOrCreate()\n",
    "    \n",
    "    \n",
    "# conf = SparkConf()\\\n",
    "# .setMaster(\"local\")\\\n",
    "# .setAppName(\"sparkmach\")\\\n",
    "# .set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "\n",
    "#sparkContext = SparkContext(conf=conf)\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/define.py\")\n",
    "#sparkSession.sparkContext.addPyFile(\"/home/vagrant/tesis/sparkmach/sparkmach/sparkmach/analyze.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/prepare.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/feature_selection.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/evaluate.py\")\n",
    "\n",
    "# STEP 0: Define workflow parameters\n",
    "definer = define.Define(sparkSession, nameData=name, className=className).pipeline()\n",
    "\n",
    "# STEP 1: Analyze data by ploting it\n",
    "#analyze.Analyze(definer).pipeline()\n",
    "\n",
    "# STEP 2: Prepare data by scaling, normalizing, etc. \n",
    "preparer = prepare.Prepare(definer).pipeline()\n",
    "\n",
    "#STEP 3: Feature selection\n",
    "featurer = feature_selection.FeatureSelection(definer).pipeline()\n",
    "\n",
    "#STEP4: Evalute the algorithms by using the pipelines\n",
    "# evaluator = evaluate.Evaluate(definer, preparer, featurer).pipeline()\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# result = main()\n",
    "# end = time.time()\n",
    "\n",
    "# print()\n",
    "# print(\"Execution time for all the steps: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+------------------+\n",
      "|Model                      |Score               |Time              |\n",
      "+---------------------------+--------------------+------------------+\n",
      "|DecisionTreeRegressor      |0.08964561627029533 |6.327453374862671 |\n",
      "|GeneralizedLinearRegression|0.013625352415245873|5.869198322296143 |\n",
      "|Total time                 |0.0                 |12.196651697158813|\n",
      "+---------------------------+--------------------+------------------+\n",
      "\n",
      "CPU times: user 444 ms, sys: 66.1 ms, total: 510 ms\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = evaluate.Evaluate(definer, preparer, featurer).pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>busID</th>\n",
       "      <th>ProximaParada</th>\n",
       "      <th>Ruta</th>\n",
       "      <th>Orientacion</th>\n",
       "      <th>rangoHora</th>\n",
       "      <th>tiempoRecorrido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3433</td>\n",
       "      <td>249</td>\n",
       "      <td>9735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3433</td>\n",
       "      <td>249</td>\n",
       "      <td>9735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3984</td>\n",
       "      <td>160</td>\n",
       "      <td>12237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3984</td>\n",
       "      <td>160</td>\n",
       "      <td>12237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5517</td>\n",
       "      <td>255</td>\n",
       "      <td>12521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   busID  ProximaParada   Ruta  Orientacion  rangoHora  tiempoRecorrido\n",
       "0   3433            249   9735          1.0        2.0            856.0\n",
       "1   3433            249   9735          1.0        5.0            636.0\n",
       "2   3984            160  12237          1.0        5.0             96.0\n",
       "3   3984            160  12237          1.0        7.5             32.0\n",
       "4   5517            255  12521          0.0       11.5             31.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_data = pd.read_csv('datasets/buses_1458098_filtered.csv')\n",
    "bus_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1449200, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.14:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sparkmach</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Sparkmach>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| ID|  c|  b|  a|\n",
      "+---+---+---+---+\n",
      "|  1|  0|  0|  1|\n",
      "|  2|  0|  1|  0|\n",
      "|  3|  1|  0|  0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = sparkSession.createDataFrame([\n",
    "    (1, \"a\"),\n",
    "    (2, \"b\"),\n",
    "    (3, \"c\"),\n",
    "], [\"ID\", \"Text\"])\n",
    "\n",
    "categories = df.select(\"Text\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "exprs = [F.when(F.col(\"Text\") == category, 1).otherwise(0).alias(category)\n",
    "         for category in categories]\n",
    "\n",
    "df.select(\"ID\", *exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'b', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(df.select(\"Text\").distinct().rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+\n",
      "|Model                      |Score               |\n",
      "+---------------------------+--------------------+\n",
      "|GBTRegressor               |0.18272008210363277 |\n",
      "|RandomForestRegressor      |0.12066847476050002 |\n",
      "|DecisionTreeRegressor      |0.08964561627029533 |\n",
      "|LinearRegression           |0.013625352415245873|\n",
      "|GeneralizedLinearRegression|0.013625352415245873|\n",
      "+---------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.sort(col(\"Score\").desc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+\n",
      "|Model                      |Score               |\n",
      "+---------------------------+--------------------+\n",
      "|LinearRegression           |0.013625352415245873|\n",
      "|GeneralizedLinearRegression|0.013625352415245873|\n",
      "|DecisionTreeRegressor      |0.08964561627029533 |\n",
      "|RandomForestRegressor      |0.12066847476050002 |\n",
      "|GBTRegressor               |0.18272008210363277 |\n",
      "+---------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Piero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88.3 ms, sys: 1.71 ms, total: 90 ms\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import bus_times\n",
    "import os\n",
    "import define\n",
    "#import analyze\n",
    "import prepare\n",
    "import feature_selection\n",
    "import evaluate\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "name = \"datasets/buses_10000_filtered.csv\"\n",
    "#     name = \"hdfs://King:9000/user/bdata/mta_data/MTA-Bus-Time_.2014-08-01.txt\"\n",
    "className = \"class\"\n",
    "\n",
    "\n",
    "sparkSession = SparkSession.builder \\\n",
    ".master('local[*]')\\\n",
    ".appName(\"Sparkmach\") \\\n",
    ".config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "#Piero\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/bus_times.py\") \n",
    "\n",
    "#Gusseppe\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/define.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/prepare.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/feature_selection.py\")\n",
    "sparkSession.sparkContext.addPyFile(currentDir + \"/evaluate.py\")\n",
    "\n",
    "rdd = sparkSession.sparkContext.textFile(currentDir + '/datasets/MTA-Bus-Time_.2014-08-01.txt')\n",
    "# rdd = sparkSession.sparkContext.textFile(currentDir + '/datasets/test.txt')\n",
    "# rdd = sc.textFile('hdfs://King:9000/user/bdata/mta_data/MTA-Bus-Time_.2014-10-31.txt')\n",
    "\n",
    "\n",
    "classTuple= bus_times.mainFilter(rdd)\n",
    "halfHourBucket=classTuple.map(lambda x: bus_times.toHalfHourBucket(list(x)))\n",
    "\n",
    "\n",
    "bucket_schema= StructType([StructField(\"bus_id\",StringType(), True),StructField(\"route_id\",StringType(), True),StructField(\"next_stop_id\",StringType(), True),StructField(\"direction\",StringType(), True),StructField(\"half_hour_bucket\",StringType(), True),StructField(\"class\",StringType(), True) ])\n",
    "# bucket_schema= StructType([StructField(\"bus_id\",IntegerType(), True),StructField(\"route_id\",StringType(), True),StructField(\"next_stop_id\",StringType(), True),StructField(\"direction\",IntegerType(), True),StructField(\"half_hour_bucket\",FloatType(), True),StructField(\"class\",FloatType(), True) ])\n",
    "\n",
    "df = sparkSession.createDataFrame(halfHourBucket, bucket_schema)\n",
    "stringIndexer = StringIndexer(inputCol='route_id', outputCol='route_id'+\"_Index\")\n",
    "df = stringIndexer.fit(df).transform(df)    \n",
    "\n",
    "stringIndexer = StringIndexer(inputCol='next_stop_id', outputCol='next_stop_id'+\"_Index\")\n",
    "df = stringIndexer.fit(df).transform(df)    \n",
    "drop_list = ['route_id', 'next_stop_id']\n",
    "\n",
    "df = df.select([column for column in df.columns if column not in drop_list])\n",
    "\n",
    "\n",
    "# print('hellllooo')\n",
    "\n",
    "# STEP 0: Define workflow parameters\n",
    "definer = define.Define(sparkSession, nameData=name, className=className, df=df).pipeline()\n",
    "\n",
    "# STEP 1: Analyze data by ploting it\n",
    "#analyze.Analyze(definer).pipeline()\n",
    "\n",
    "# STEP 2: Prepare data by scaling, normalizing, etc. \n",
    "preparer = prepare.Prepare(definer).pipeline()\n",
    "\n",
    "#STEP 3: Feature selection\n",
    "featurer = feature_selection.FeatureSelection(definer).pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+------------------+\n",
      "|Model                |Score              |Time              |\n",
      "+---------------------+-------------------+------------------+\n",
      "|DecisionTreeRegressor|0.05254448412691726|494.01554322242737|\n",
      "|Total time           |0.0                |494.01554322242737|\n",
      "+---------------------+-------------------+------------------+\n",
      "\n",
      "CPU times: user 506 ms, sys: 119 ms, total: 625 ms\n",
      "Wall time: 8min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = evaluate.Evaluate(definer, preparer, featurer).pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-120f76ddb124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "sparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringIndexer = StringIndexer(inputCol='route_id', outputCol='route_id'+\"_Index\")\n",
    "# df = stringIndexer.fit(times_df).transform(times_df)    \n",
    "\n",
    "# stringIndexer = StringIndexer(inputCol='next_stop_id', outputCol='next_stop_id'+\"_Index\")\n",
    "# result = stringIndexer.fit(df).transform(df)    \n",
    "# drop_list = ['route_id', 'next_stop_id']\n",
    "\n",
    "# df = df.select([column for column in df.columns if column not in drop_list])\n",
    "\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RandomForestClassificationModel.load(\"./models/name.ml\")\n",
    "\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-81601c7341cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator.Evaluate.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(bus_id,FloatType,true),\n",
       " StructField(direction,FloatType,true),\n",
       " StructField(half_hour_bucket,FloatType,true),\n",
       " StructField(class,FloatType,true),\n",
       " StructField(route_id_Index,DoubleType,true),\n",
       " StructField(next_stop_id_Index,DoubleType,true)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definer.data.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449761"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definer.data.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449761"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = definer.data.dropna()\n",
    "tt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'definer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eb73c5e8922b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdefiner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'definer' is not defined"
     ]
    }
   ],
   "source": [
    "definer.data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(bus_id,StringType,true),StructField(direction,StringType,true),StructField(half_hour_bucket,StringType,true),StructField(class,FloatType,true),StructField(route_id_Index,DoubleType,true)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df  = df.withColumn(\"class\", col(\"class\").cast('float'))\n",
    "df  = df.withColumn(\"bus_id\", col(\"class\").cast('float'))\n",
    "df  = df.withColumn(\"direction\", col(\"class\").cast('float'))\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+-----+--------------+\n",
      "|bus_id|direction|half_hour_bucket|class|route_id_Index|\n",
      "+------+---------+----------------+-----+--------------+\n",
      "|  4367|        1|             9.0|  0.0|         157.0|\n",
      "|  4367|        1|            10.0|157.0|         157.0|\n",
      "|  4991|        1|             9.0| 63.0|         119.0|\n",
      "|   230|        0|             9.0| 32.0|           9.0|\n",
      "+------+---------+----------------+-----+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rawdata.select(col('house name'), rawdata.price.cast('float').alias('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458098"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_df.select('next_stop_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_df.select('route_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# times_df.write.csv('datasets/bus_2014-08-01.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = sparkSession.read\\\n",
    "                .format(\"txt\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "                .option(\"delimiter\", \"\\t\")\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .csv('datasets/MTA-Bus-Time_.2014-08-01.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458098"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+---------+----------------+-----+\n",
      "|bus_id|     route_id|next_stop_id|direction|half_hour_bucket|class|\n",
      "+------+-------------+------------+---------+----------------+-----+\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|             9.0|  0.0|\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|            10.0|157.0|\n",
      "|  4991| MTA NYCT_B83|  MTA_308528|        1|             9.0| 63.0|\n",
      "|   230| MTA NYCT_B35|  MTA_302697|        0|             9.0| 32.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|             9.0| 31.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|            10.5|    0|\n",
      "|  3742|    MTABC_Q10|  MTA_550316|        1|             9.0| 32.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             9.0| 31.0|\n",
      "+------+-------------+------------+---------+----------------+-----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times_df_t = times_df.limit(40)\n",
    "times_df_t.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|     route_id|next_stop_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|             9.0|  0.0|           7.0|               5.0|\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|            10.0|157.0|           7.0|               5.0|\n",
      "|  4991| MTA NYCT_B83|  MTA_308528|        1|             9.0| 63.0|          14.0|              10.0|\n",
      "|   230| MTA NYCT_B35|  MTA_302697|        0|             9.0| 32.0|          10.0|              17.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|             9.0| 31.0|           3.0|               8.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|            10.5|    0|           3.0|               8.0|\n",
      "|  3742|    MTABC_Q10|  MTA_550316|        1|             9.0| 32.0|           9.0|              12.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             9.0| 31.0|           4.0|               3.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             1.0|728.0|           4.0|               3.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             3.0|250.0|           4.0|               3.0|\n",
      "|   393| MTA NYCT_B68|  MTA_305700|        0|             9.0| 31.0|          13.0|              14.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             9.0|  0.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|            16.5| 95.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             3.0| 93.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             6.5| 31.0|           2.0|               2.0|\n",
      "|  3706|    MTABC_Q52|  MTA_550441|        0|             9.0| 62.0|          11.0|              15.0|\n",
      "|  4762|MTA NYCT_BX40|  MTA_103525|        1|             9.0| 62.0|          12.0|              18.0|\n",
      "|  3400|   MTABC_BXM9|  MTA_450082|        0|             9.0|655.0|           6.0|               7.0|\n",
      "|  3400|   MTABC_BXM9|  MTA_450082|        0|             6.0|988.0|           6.0|               7.0|\n",
      "|  6365| MTA NYCT_M10|  MTA_401335|        1|             9.0| 32.0|           3.0|              13.0|\n",
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol='route_id', outputCol='route_id'+\"_Index\")\n",
    "result = stringIndexer.fit(times_df_t).transform(times_df_t)    \n",
    "\n",
    "stringIndexer = StringIndexer(inputCol='next_stop_id', outputCol='next_stop_id'+\"_Index\")\n",
    "result = stringIndexer.fit(result).transform(result)    \n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|     route_id|next_stop_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|             9.0|  0.0|           7.0|               5.0|\n",
      "|  4367|MTA NYCT_BX13|  MTA_100897|        1|            10.0|157.0|           7.0|               5.0|\n",
      "|  4991| MTA NYCT_B83|  MTA_308528|        1|             9.0| 63.0|          14.0|              10.0|\n",
      "|   230| MTA NYCT_B35|  MTA_302697|        0|             9.0| 32.0|          10.0|              17.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|             9.0| 31.0|           3.0|               8.0|\n",
      "|  3834| MTA NYCT_M10|  MTA_401257|        0|            10.5|    0|           3.0|               8.0|\n",
      "|  3742|    MTABC_Q10|  MTA_550316|        1|             9.0| 32.0|           9.0|              12.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             9.0| 31.0|           4.0|               3.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             1.0|728.0|           4.0|               3.0|\n",
      "|  6296| MTA NYCT_S44|  MTA_201256|        1|             3.0|250.0|           4.0|               3.0|\n",
      "|   393| MTA NYCT_B68|  MTA_305700|        0|             9.0| 31.0|          13.0|              14.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             9.0|  0.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|            16.5| 95.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             3.0| 93.0|           2.0|               2.0|\n",
      "|   496|   MTABC_B103|  MTA_304072|        0|             6.5| 31.0|           2.0|               2.0|\n",
      "|  3706|    MTABC_Q52|  MTA_550441|        0|             9.0| 62.0|          11.0|              15.0|\n",
      "|  4762|MTA NYCT_BX40|  MTA_103525|        1|             9.0| 62.0|          12.0|              18.0|\n",
      "|  3400|   MTABC_BXM9|  MTA_450082|        0|             9.0|655.0|           6.0|               7.0|\n",
      "|  3400|   MTABC_BXM9|  MTA_450082|        0|             6.0|988.0|           6.0|               7.0|\n",
      "|  6365| MTA NYCT_M10|  MTA_401335|        1|             9.0| 32.0|           3.0|              13.0|\n",
      "+------+-------------+------------+---------+----------------+-----+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "|  4367|        1|             9.0|  0.0|           7.0|               5.0|\n",
      "|  4367|        1|            10.0|157.0|           7.0|               5.0|\n",
      "|  4991|        1|             9.0| 63.0|          14.0|              10.0|\n",
      "|   230|        0|             9.0| 32.0|          10.0|              17.0|\n",
      "|  3834|        0|             9.0| 31.0|           3.0|               8.0|\n",
      "|  3834|        0|            10.5|    0|           3.0|               8.0|\n",
      "|  3742|        1|             9.0| 32.0|           9.0|              12.0|\n",
      "|  6296|        1|             9.0| 31.0|           4.0|               3.0|\n",
      "|  6296|        1|             1.0|728.0|           4.0|               3.0|\n",
      "|  6296|        1|             3.0|250.0|           4.0|               3.0|\n",
      "|   393|        0|             9.0| 31.0|          13.0|              14.0|\n",
      "|   496|        0|             9.0|  0.0|           2.0|               2.0|\n",
      "|   496|        0|            16.5| 95.0|           2.0|               2.0|\n",
      "|   496|        0|             3.0| 93.0|           2.0|               2.0|\n",
      "|   496|        0|             6.5| 31.0|           2.0|               2.0|\n",
      "|  3706|        0|             9.0| 62.0|          11.0|              15.0|\n",
      "|  4762|        1|             9.0| 62.0|          12.0|              18.0|\n",
      "|  3400|        0|             9.0|655.0|           6.0|               7.0|\n",
      "|  3400|        0|             6.0|988.0|           6.0|               7.0|\n",
      "|  6365|        1|             9.0| 32.0|           3.0|              13.0|\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_list = ['route_id', 'next_stop_id']\n",
    "\n",
    "result = result.select([column for column in result.columns if column not in drop_list])\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.2581, 0.7419]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9186, 0.0814]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.432, 0.568]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.6766, 0.3234]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = sparkSession.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = sparkSession.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.save('./models/bm.ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = CrossValidator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_4ef7b1a9cd490897dff4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = PipelineModel.load('./models/bm.ml')\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|           text|             words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|    spark i j k|  [spark, i, j, k]|(100,[5,29,49,56]...|[-1.0560322733153...|[0.25806842225846...|       1.0|\n",
      "|  5|          l m n|         [l, m, n]|(100,[6,38,55],[1...|[2.42293721495130...|[0.91855974126539...|       0.0|\n",
      "|  6|mapreduce spark|[mapreduce, spark]|(100,[5,53],[1.0,...|[-0.2735651887090...|[0.43203205663918...|       1.0|\n",
      "|  7|  apache hadoop|  [apache, hadoop]|(100,[81,95],[1.0...|[0.73822817597493...|[0.67660828566522...|       0.0|\n",
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp.transform(test).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|           text|             words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|    spark i j k|  [spark, i, j, k]|(100,[5,29,49,56]...|[-1.0560322733153...|[0.25806842225846...|       1.0|\n",
      "|  5|          l m n|         [l, m, n]|(100,[6,38,55],[1...|[2.42293721495130...|[0.91855974126539...|       0.0|\n",
      "|  6|mapreduce spark|[mapreduce, spark]|(100,[5,53],[1.0,...|[-0.2735651887090...|[0.43203205663918...|       1.0|\n",
      "|  7|  apache hadoop|  [apache, hadoop]|(100,[81,95],[1.0...|[0.73822817597493...|[0.67660828566522...|       0.0|\n",
      "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "pp = PipelineModel.load('./models/DecisionTreeRegressor.ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_44d58fdf5cfb5440246c,\n",
       " StandardScaler_4112993244769ab8b3da,\n",
       " DecisionTreeRegressionModel (uid=DecisionTreeRegressor_48ea9eb0e7619d1ff934) of depth 5 with 63 nodes]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "|bus_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "| 174.0|      0.0|             0.5| 32.0|         208.0|             144.0|\n",
      "| 174.0|      1.0|            15.0|  0.0|         208.0|             185.0|\n",
      "| 174.0|      1.0|            18.0| 32.0|         208.0|             185.0|\n",
      "| 174.0|      1.0|            23.0| 29.0|         208.0|             177.0|\n",
      "| 185.0|      0.0|             2.0| 31.0|          18.0|            3713.0|\n",
      "| 185.0|      0.0|             2.5|155.0|          18.0|               4.0|\n",
      "| 185.0|      0.0|             7.0| 31.0|          18.0|            1479.0|\n",
      "| 185.0|      0.0|             7.0| 95.0|          18.0|            3713.0|\n",
      "| 185.0|      0.0|            17.5| 32.0|          18.0|            1479.0|\n",
      "| 185.0|      0.0|            18.0|  0.0|          18.0|           10932.0|\n",
      "+------+---------+----------------+-----+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator.test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+-----+--------------+------------------+--------------------+--------------------+------------------+\n",
      "|bus_id|direction|half_hour_bucket|class|route_id_Index|next_stop_id_Index|            features|      scaledFeatures|        prediction|\n",
      "+------+---------+----------------+-----+--------------+------------------+--------------------+--------------------+------------------+\n",
      "| 174.0|      0.0|             0.5| 32.0|         208.0|             144.0|[174.0,0.0,0.5,20...|[-2.0292309107064...|244.84900166389352|\n",
      "| 174.0|      1.0|            15.0|  0.0|         208.0|             185.0|[174.0,1.0,15.0,2...|[-2.0292309107064...|244.84900166389352|\n",
      "| 174.0|      1.0|            18.0| 32.0|         208.0|             185.0|[174.0,1.0,18.0,2...|[-2.0292309107064...|244.84900166389352|\n",
      "| 174.0|      1.0|            23.0| 29.0|         208.0|             177.0|[174.0,1.0,23.0,2...|[-2.0292309107064...|244.84900166389352|\n",
      "| 185.0|      0.0|             2.0| 31.0|          18.0|            3713.0|[185.0,0.0,2.0,18...|[-2.0246647123644...| 62.01413425868233|\n",
      "| 185.0|      0.0|             2.5|155.0|          18.0|               4.0|[185.0,0.0,2.5,18...|[-2.0246647123644...|142.43136673584345|\n",
      "| 185.0|      0.0|             7.0| 31.0|          18.0|            1479.0|[185.0,0.0,7.0,18...|[-2.0246647123644...|101.85339750035915|\n",
      "| 185.0|      0.0|             7.0| 95.0|          18.0|            3713.0|[185.0,0.0,7.0,18...|[-2.0246647123644...| 62.01413425868233|\n",
      "| 185.0|      0.0|            17.5| 32.0|          18.0|            1479.0|[185.0,0.0,17.5,1...|[-2.0246647123644...|101.85339750035915|\n",
      "| 185.0|      0.0|            18.0|  0.0|          18.0|           10932.0|[185.0,0.0,18.0,1...|[-2.0246647123644...| 41.92107007010987|\n",
      "+------+---------+----------------+-----+--------------+------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = pp.transform(evaluator.test.limit(10))\n",
    "\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"total_bill\", y=\"tip\", data=tips)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
