{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0435d6271224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#name = \"hdfs://King:9000/user/bdata/cern/hepmass_2000000_report.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# current_path = os.getcwd()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcurrent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hepmass.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import bus_times\n",
    "import os\n",
    "import define\n",
    "#import analyze\n",
    "import prepare\n",
    "import feature_selection\n",
    "import evaluate\n",
    "import tools\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#name = \"datasets/buses_10000_filtered.csv\"\n",
    "#name = \"hdfs://King:9000/user/bdata/cern/hepmass_2000000_report.csv\"\n",
    "current_path = os.getcwd()\n",
    "#current_path = os.path.dirname(os.path.abspath(__file__))\n",
    "print(current_path)\n",
    "data_path = os.path.join(current_path, 'hepmass.csv')\n",
    "response = \"label\"\n",
    "#cluster_manager = 'yarn'\n",
    "cluster_manager = 'local[*]'\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    ".master(cluster_manager)\\\n",
    ".appName(\"Sparkmach\") \\\n",
    ".config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "\n",
    "spark_session.sparkContext.addPyFile(os.path.join(current_path, 'define.py'))\n",
    "spark_session.sparkContext.addPyFile(os.path.join(current_path, 'prepare.py'))\n",
    "spark_session.sparkContext.addPyFile(os.path.join(current_path, 'feature_selection.py'))\n",
    "spark_session.sparkContext.addPyFile(os.path.join(current_path, 'evaluate.py'))\n",
    "spark_session.sparkContext.addPyFile(os.path.join(current_path, 'tools.py'))\n",
    "##### Benchmark starting #####\n",
    "print('Benchmark starting:')\n",
    "\n",
    "list_n_samples = [10, 100, 1000]\n",
    "list_n_features = [5, 10, 15]\n",
    "print('List number of samples:', list_n_samples)\n",
    "print('List number of features:', list_n_features)\n",
    "\n",
    "for n_samples in list_n_samples:\n",
    "    for n_features in list_n_features:\n",
    "        \n",
    "        ##### Generate the dataframe #####\n",
    "        print('Generating the binary labeled dataframe: shape:', n_samples, n_features)\n",
    "        df = tools.generate_dataframe(spark_session, n_samples=n_samples, \n",
    "                                      n_features=n_features, seed=42)\n",
    "        # df.show(3)\n",
    "\n",
    "        ##### Run the models #####\n",
    "        print('Running the models')\n",
    "\n",
    "        # STEP 0: Define workflow parameters\n",
    "        #definer = define.Define(spark_session, data_path=data_path, response=response).pipeline()\n",
    "        definer = define.Define(spark_session, df=df, response='response').pipeline()\n",
    "\n",
    "        # STEP 1: Analyze data by ploting it\n",
    "        #analyze.Analyze(definer).pipeline()\n",
    "\n",
    "        # STEP 2: Prepare data by scaling, normalizing, etc. \n",
    "        preparer = prepare.Prepare(definer).pipeline()\n",
    "\n",
    "        #STEP 3: Feature selection\n",
    "        featurer = feature_selection.FeatureSelection(definer).pipeline()\n",
    "\n",
    "        #STEP4: Evalute the algorithms by using the pipelines\n",
    "        evaluator = evaluate.Evaluate(definer, preparer, featurer).pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3493514815966288"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.11530292828877767 + 0.23404855330785115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6881be1e133e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sparkmach</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd04bb40dd8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definer.da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, randn, when\n",
    "\n",
    "def generate_dataframe(n_samples, n_features, seed=42):\n",
    "    df = spark_session.range(0, n_samples)\n",
    "    t = [when(df['id'] < n_samples/2, 0).otherwise(1).alias(\"response\")]\n",
    "    t += [rand(seed=seed+i).alias(\"f_\"+str(i)) for i in range(n_features)]\n",
    "    df = df.select(t)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+-------------------+\n",
      "|response|                f_0|                f_1|                f_2|\n",
      "+--------+-------------------+-------------------+-------------------+\n",
      "|       0| 0.6661236774413726| 0.3856203005100328|0.47129200262114224|\n",
      "|       0| 0.3856203005100328|0.47129200262114224|0.46779322384305533|\n",
      "|       0|0.47129200262114224|0.46779322384305533| 0.8244413413402464|\n",
      "|       0|0.46779322384305533| 0.8244413413402464|0.26547180193280995|\n",
      "|       0| 0.8826789945738182| 0.5750003757719865|0.22508188677695096|\n",
      "|       1| 0.8244413413402464|0.26547180193280995| 0.8642043127063618|\n",
      "|       1|0.26547180193280995| 0.8642043127063618| 0.8401446809670715|\n",
      "|       1| 0.8642043127063618| 0.8401446809670715| 0.5665388805573012|\n",
      "|       1| 0.8401446809670715| 0.5665388805573012|  0.748698453025968|\n",
      "|       1| 0.3562475874559726| 0.9021459462713273| 0.5736208594213481|\n",
      "+--------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = generate_dataframe(n_samples=10, n_features=3, seed=42)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
